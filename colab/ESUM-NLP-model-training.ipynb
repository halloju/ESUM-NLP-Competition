{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ESUM-NLP-model-training.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":["XBNRy9tXE9vC","pg5P_gReO6qI","5xyP_ClUOeLF"],"toc_visible":true,"authorship_tag":"ABX9TyMqeh3hPa7K6qxHkQoS1QzR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Isq8xtSsEbzz","colab_type":"text"},"source":["### Mount Data"]},{"cell_type":"code","metadata":{"id":"dmxcSuMIEAb4","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OLttZhdAnNB-","colab_type":"code","colab":{}},"source":["!pip install -r '/content/drive/My Drive/ESUM-NLP-Competition/requirements.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zLNLOpF2EpOy","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","import os\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import transformers\n","\n","from tokenizers import BertWordPieceTokenizer\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","from plotly import graph_objs as go\n","import plotly.express as px\n","import plotly.figure_factory as ff"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NQc-e83O55IK","colab_type":"text"},"source":["### AML Classification"]},{"cell_type":"code","metadata":{"id":"g1Kd2xBWUzA-","colab_type":"code","colab":{}},"source":["data = pd.read_csv('/content/drive/My Drive/ESUM-NLP-Competition/data/all.csv')\n","data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NAb7K6qlm7Ff","colab_type":"code","colab":{}},"source":["xtrain, xvalid, ytrain, yvalid = train_test_split(data.news.values, data.IND.values, \n","                                                  stratify=data.IND.values, \n","                                                  random_state=101, \n","                                                  test_size=0.2, shuffle=True)                                                "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oD4_CZklUMav","colab_type":"code","colab":{}},"source":["import transformers\n","def fast_encode(texts, tokenizer, chunk_size=256, maxlen=192):\n","    \"\"\"\n","    Encoder for encoding the text into sequence of integers for BERT Input\n","    \"\"\"\n","    tokenizer.enable_truncation(max_length=maxlen)\n","    tokenizer.enable_padding(length=maxlen)\n","    all_ids = []\n","\n","    text_chunk = [texts]\n","    encs = tokenizer.encode_batch(text_chunk)\n","    all_ids.extend([enc.ids for enc in encs])\n","    \n","    return np.array(all_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5p6vaSmjNK4D","colab_type":"code","colab":{}},"source":["tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-chinese\", from_pt = True)\n","\n","# Save the loaded tokenizer locally\n","tokenizer.save_pretrained('.')\n","# Reload it with the huggingface tokenizers library\n","fast_tokenizer = BertWordPieceTokenizer('./vocab.txt', lowercase=False)\n","fast_tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y6h2cM8H2J0s","colab_type":"code","colab":{}},"source":["from tensorflow.keras import backend as K\n","\n","def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","def precision_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","def f1_m(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_5QrVELRfl86","colab_type":"code","colab":{}},"source":["def build_model(transformer, max_len=128):\n","    \"\"\"\n","    function for training the BERT model\n","    \"\"\"\n","    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n","    sequence_output = transformer(input_word_ids)[0]\n","    cls_token = sequence_output[:, 0, :]\n","    out = Dense(1, activation='sigmoid')(cls_token)\n","    \n","    model = Model(inputs=input_word_ids, outputs=out)\n","    model.compile(Adam(lr=1e-5), loss=\"binary_crossentropy\", metrics=['accuracy',f1_m,precision_m, recall_m])\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mkVwfXG9nqty","colab_type":"code","colab":{}},"source":["#IMP DATA FOR CONFIG\n","\n","AUTO = tf.data.experimental.AUTOTUNE\n","train_dataset = (\n","    tf.data.Dataset\n","    .from_tensor_slices((x_train, y_train))\n","    .repeat()\n","    .shuffle(2048)\n","    .batch(BATCH_SIZE)\n","    .prefetch(AUTO)\n",")\n","\n","\n","valid_dataset = (\n","    tf.data.Dataset\n","    .from_tensor_slices((x_valid, y_valid))\n","    .batch(BATCH_SIZE)\n","    .cache()\n","    .prefetch(AUTO)\n",")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V9HgncuYXY7t","colab_type":"code","colab":{}},"source":["EPOCHS = 12\n","BATCH_SIZE = 16\n","MAX_LEN = 224"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YGRo8s0ll6QQ","colab_type":"code","colab":{}},"source":["%%time\n","strategy = tf.distribute.get_strategy()\n","with strategy.scope():\n","    transformer_layer = (\n","        transformers.TFBertModel\n","        .from_pretrained('bert-base-chinese' ,from_pt = True)\n","    )\n","    cls = build_model(transformer_layer, max_len=MAX_LEN)\n","cls.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5msR7NFWn7lI","colab_type":"code","colab":{}},"source":["from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","\n","early_stop = EarlyStopping(patience=3)\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=1, mode='auto')\n","\n","n_steps = x_train.shape[0] // BATCH_SIZE\n","train_history = cls.fit(\n","    train_dataset,\n","    steps_per_epoch=n_steps,\n","    validation_data=valid_dataset,\n","    epochs=EPOCHS,\n","    callbacks=[early_stop, reduce_lr]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"walDLaqMT89k","colab_type":"code","colab":{}},"source":["train_history_2 = cls.fit(\n","    valid_dataset.repeat(),\n","    steps_per_epoch=n_steps,\n","    epochs=2\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KsKOuGl2a8Ge","colab_type":"code","colab":{}},"source":["cls.save_weights('/content/drive/My Drive/ESUM-NLP-Competition/model/bert-Classification-model-224', save_format='h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uIuPXoV7BCeF","colab_type":"text"},"source":["### Sentence selection"]},{"cell_type":"code","metadata":{"id":"drBeZjPiSSOB","colab_type":"code","colab":{}},"source":["def sents_encode(texts, tokenizer, chunk_size=32, maxlen=64):\n","    \"\"\"\n","    Encoder for encoding the text into sequence of integers for BERT Input\n","    \"\"\"\n","    tokenizer.enable_truncation(max_length=maxlen)\n","    tokenizer.enable_padding(length=maxlen)\n","    all_ids = []\n","    \n","    for i in range(0, len(texts), chunk_size):\n","        text_chunk = texts[i:i+chunk_size]\n","        encs = tokenizer.encode_batch(text_chunk)\n","        all_ids.extend([enc.ids for enc in encs])\n","    \n","    return np.array(all_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yduIt9peWo97","colab_type":"code","colab":{}},"source":["x_df_ = pd.read_csv('/content/drive/My Drive/ESUM-NLP-Competition/data/x_sents_1.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"REMwBZ6ZmxVy","colab_type":"code","colab":{}},"source":["xtrain, xvalid, ytrain, yvalid = train_test_split(x_df_.news.values, x_df_.IND.values, \n","                                                  stratify=x_df_.IND.values, \n","                                                  random_state=1010, \n","                                                  test_size=0.2, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uwRXvuecXG8Z","colab_type":"code","colab":{}},"source":["xtrain = sents_encode(xtrain, fast_tokenizer, maxlen=64)\n","xvalid = sents_encode(xvalid, fast_tokenizer, maxlen=64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hpnI75rTnm1f","colab_type":"code","colab":{}},"source":["BATCH_SIZE = 64\n","AUTO = tf.data.experimental.AUTOTUNE\n","\n","train_dataset = (\n","    tf.data.Dataset\n","    .from_tensor_slices((xtrain, ytrain))\n","    .repeat()\n","    .shuffle(2048)\n","    .batch(BATCH_SIZE)\n","    .prefetch(AUTO)\n",")\n","\n","\n","valid_dataset = (\n","    tf.data.Dataset\n","    .from_tensor_slices((xvalid, yvalid))\n","    .batch(BATCH_SIZE)\n","    .cache()\n","    .prefetch(AUTO)\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eLrVmM72W6YR","colab_type":"code","colab":{}},"source":["strategy = tf.distribute.get_strategy()\n","with strategy.scope():\n","    transformer_layer = (\n","        transformers.TFBertModel\n","        .from_pretrained('bert-base-chinese' ,from_pt = True)\n","    )\n","    csent = build_model(transformer_layer, max_len=64)\n","csent.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IYCLzrGY1l6M","colab_type":"code","colab":{}},"source":["n_steps = xtrain.shape[0] // BATCH_SIZE\n","train_history = csent.fit(\n","    train_dataset,\n","    steps_per_epoch=n_steps,\n","    validation_data=valid_dataset,\n","    epochs=EPOCHS,\n","    callbacks=[early_stop, reduce_lr]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k1n01sP3rMle","colab_type":"code","colab":{}},"source":["train_history_2 = csent.fit(\n","    valid_dataset.repeat(),\n","    steps_per_epoch=n_steps,\n","    epochs=1\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-zbklPY_XdC","colab_type":"code","colab":{}},"source":["csent.save_weights('/content/drive/My Drive/ESUM-NLP-Competition/model/Sents-Classification-model-64', save_format='h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XBNRy9tXE9vC","colab_type":"text"},"source":["### BiGRU Model(NER)"]},{"cell_type":"code","metadata":{"id":"yNUQPBAyFVXS","colab_type":"code","colab":{}},"source":["import kashgari\n","from kashgari.tasks.labeling import BiGRU_Model\n","from kashgari.embeddings import BertEmbedding\n","\n","\n","SEQUENCE_LENGTH = 100\n","EPOCHS = 30\n","EARL_STOPPING_PATIENCE = 5\n","REDUCE_RL_PATIENCE = 3\n","\n","BATCH_SIZE = 64\n","BERT_PATH = '/content/drive/My Drive/ESUM-NLP-Competition/bert-zh/chinese_wwm_ext_L-12_H-768_A-12'\n","embed = BertEmbedding(BERT_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AFaPPtvqvUmF","colab_type":"code","colab":{}},"source":["from kashgari.corpus import ChineseDailyNerCorpus\n","import kashgari\n","\n","train_x, train_y = ChineseDailyNerCorpus.load_data('train')\n","test_x, test_y = ChineseDailyNerCorpus.load_data('test')\n","valid_x, valid_y = ChineseDailyNerCorpus.load_data('valid')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gIJJdpRpcvaW","colab_type":"code","colab":{}},"source":["model = BiGRU_Model(embed, sequence_length=SEQUENCE_LENGTH)\n","          \n","early_stop = keras.callbacks.EarlyStopping(patience=EARL_STOPPING_PATIENCE)\n","reduse_lr_callback = keras.callbacks.ReduceLROnPlateau(factor=0.1, \n","                                                        patience=2)\n","print(model.sequence_length)\n","eval_callback = EvalCallBack(kash_model=model,\n","                              x_data=valid_x, \n","                              y_data=valid_y,\n","                              truncating=True,\n","                              step=2)\n","\n","tf_board = keras.callbacks.TensorBoard(\n","    log_dir=os.path.join(TF_LOG_FOLDER, run_name), \n","    update_freq=1000\n",")\n","\n","callbacks = [early_stop, reduse_lr_callback, eval_callback, tf_board]\n","\n","model.build_model(train_x, train_y)\n","model.compile_model(optimizer=Adam(lr=1e-5))\n","model.fit(train_x, train_y, valid_x, valid_y,\n","          callbacks=callbacks,\n","          epochs=EPOCHS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0WH1oInt904F","colab_type":"code","colab":{}},"source":["model.save(\"/content/drive/My Drive//ESUM-NLP-Competition/model/Bert-Chinese_BiGRU_Model-100\")"],"execution_count":null,"outputs":[]}]}